services:
  friend-backend:
    image: ghcr.io/AnkushMalaker/friend-lite/friend-backend:latest
    ports:
      - "8000:8000"
    env_file:
      - backends/advanced-backend/.env
    volumes:
      - ./data/audio_chunks:/app/audio_chunks
      - ./data/debug_dir:/app/debug_dir
      - ./data:/app/data
    environment:
      # Core backend settings
      - HOST=${HOST:-0.0.0.0}
      - PORT=${PORT:-8000}
      - DEBUG=${DEBUG:-false}
      - DEBUG_DUMP_DIR=${DEBUG_DUMP_DIR:-debug_dumps}
      
      # Database connections
      - MONGODB_URI=${MONGODB_URI:-mongodb://mongo:27017}
      - QDRANT_BASE_URL=${QDRANT_BASE_URL:-qdrant}
      
      # Audio processing
      - NEW_CONVERSATION_TIMEOUT_MINUTES=${NEW_CONVERSATION_TIMEOUT_MINUTES:-1.5}
      - AUDIO_CROPPING_ENABLED=${AUDIO_CROPPING_ENABLED:-true}
      - MIN_SPEECH_SEGMENT_DURATION=${MIN_SPEECH_SEGMENT_DURATION:-1.0}
      - CROPPING_CONTEXT_PADDING=${CROPPING_CONTEXT_PADDING:-0.1}
      
      # Authentication (required)
      - AUTH_SECRET_KEY=${AUTH_SECRET_KEY}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}
      - ADMIN_EMAIL=${ADMIN_EMAIL:-admin@example.com}
      - COOKIE_SECURE=${COOKIE_SECURE:-false}
      
      # Transcription providers
      - TRANSCRIPTION_PROVIDER=${TRANSCRIPTION_PROVIDER}
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - MISTRAL_MODEL=${MISTRAL_MODEL:-voxtral-mini-2507}
      - OFFLINE_ASR_TCP_URI=${OFFLINE_ASR_TCP_URI:-tcp://localhost:8765}
      
      # LLM providers
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o}
      - OPENAI_EMBEDDER_MODEL=${OPENAI_EMBEDDER_MODEL:-text-embedding-3-small}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - OLLAMA_EMBEDDER_MODEL=${OLLAMA_EMBEDDER_MODEL:-nomic-embed-text:latest}
      
      # Memory services
      - NEO4J_HOST=${NEO4J_HOST:-neo4j-mem0}
      - NEO4J_USER=${NEO4J_USER:-neo4j}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
      - MEM0_TELEMETRY=${MEM0_TELEMETRY:-false}
      - MEM0_ORGANIZATION_ID=${MEM0_ORGANIZATION_ID:-friend-lite-org}
      - MEM0_PROJECT_ID=${MEM0_PROJECT_ID:-audio-conversations}
      - MEM0_APP_ID=${MEM0_APP_ID:-omi-backend}
      
      # Additional services
      - SPEAKER_SERVICE_URL=${SPEAKER_SERVICE_URL}
      - HF_TOKEN=${HF_TOKEN}
    depends_on:
      qdrant:
        condition: service_started
      mongo:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/readiness"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  streamlit:
    image: ghcr.io/AnkushMalaker/friend-lite/friend-webui:latest
    ports:
      - "8501:8501"
    environment:
      - BACKEND_API_URL=http://friend-backend:8000
      - BACKEND_PUBLIC_URL=${BACKEND_PUBLIC_URL:-http://localhost:8000}
      - STREAMLIT_SERVER_ENABLE_CORS=false
    depends_on:
      friend-backend:
        condition: service_healthy
      mongo:
        condition: service_started
      qdrant:
        condition: service_started

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333" # gRPC
      - "6334:6334" # HTTP
    volumes:
      - ./data/qdrant_data:/qdrant/storage 

  mongo:
    image: mongo:4.4.18
    ports:
      - "27017:27017"
    volumes:
      - ./data/mongo_data:/data/db

  # Optional services (uncomment to enable)

  # neo4j-mem0:
  #   image: neo4j:5.15-community
  #   ports:
  #     - "7474:7474" # HTTP
  #     - "7687:7687" # Bolt
  #   environment:
  #     - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD:-password}
  #     - NEO4J_PLUGINS=["apoc"]
  #     - NEO4J_dbms_security_procedures_unrestricted=apoc.*
  #     - NEO4J_dbms_security_procedures_allowlist=apoc.*
  #   volumes:
  #     - ./data/neo4j_data:/data
  #     - ./data/neo4j_logs:/logs
  #   restart: unless-stopped

  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]

  # nginx:
  #   image: nginx:alpine
  #   depends_on: [friend-backend, streamlit]
  #   volumes:
  #     - ./backends/advanced-backend/nginx.conf:/etc/nginx/nginx.conf:ro
  #   ports: 
  #     - "80:80"

# volumes:
#   ollama_data:
#     driver: local 