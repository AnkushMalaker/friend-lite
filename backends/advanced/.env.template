# ========================================
# GETTING STARTED
# ========================================
# 1. Copy this file to .env: cp .env.template .env
# 2. Fill in your API keys below (at minimum: DEEPGRAM_API_KEY, OPENAI_API_KEY)
# 3. Run: docker compose up --build -d
# 4. For testing: ./run-test.sh (requires API keys to be set)

# This key is used to sign your JWT token, just make it random and long
AUTH_SECRET_KEY=

# This is the password for the admin user
ADMIN_PASSWORD=

# Admin email (defaults to admin@example.com if not set)
ADMIN_EMAIL=admin@example.com

# ========================================
# LLM CONFIGURATION (Standard)
# ========================================

# LLM Provider: "openai" or "ollama" (default: openai)
LLM_PROVIDER=openai

# OpenAI or OpenAI-compatible API configuration
OPENAI_API_KEY=your-openai-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini

# For Ollama (OpenAI-compatible mode):
# LLM_PROVIDER=ollama
# OLLAMA_BASE_URL=dummy
# OLLAMA_BASE_URL=http://ollama:11434/v1
# OLLAMA_MODEL=llama3.1:latest
# OLLAMA_EMBEDDER_MODEL=nomic-embed-text:latest

# ========================================
# CHAT INTERFACE CONFIGURATION (Optional)
# ========================================

# Chat-specific LLM model (defaults to OPENAI_MODEL if not set)
# CHAT_LLM_MODEL=gpt-4o-mini

# Chat temperature for more conversational responses (defaults to 0.7)
# CHAT_TEMPERATURE=0.7

# ========================================
# SPEECH-TO-TEXT CONFIGURATION (Choose one)
# ========================================

# Option 1: Deepgram (recommended for best transcription quality)
DEEPGRAM_API_KEY=

# Option 2: Parakeet ASR service from extras/asr-services
# PARAKEET_ASR_URL=http://host.docker.internal:8767

# Optional: Specify which provider to use ('deepgram' or 'parakeet')
# If not set, will auto-select based on available configuration (Deepgram preferred)
# TRANSCRIPTION_PROVIDER=

# ========================================
# SPEECH DETECTION CONFIGURATION
# ========================================

# Speech detection settings for conversation creation (speech-driven architecture)
# Only meaningful speech creates conversations - silence/noise is filtered out

# Minimum words required to create a conversation (default: 5)
SPEECH_DETECTION_MIN_WORDS=5

# Minimum word confidence threshold (0.0-1.0, default: 0.5)
# Used for both conversation creation and speech gap analysis
SPEECH_DETECTION_MIN_CONFIDENCE=0.5

# Batch transcription monitoring (for batch providers like Parakeet)
TRANSCRIPTION_BUFFER_SECONDS=120    # Trigger transcription every N seconds

# Auto-stop thresholds
SPEECH_INACTIVITY_THRESHOLD_SECONDS=60  # Close conversation after N seconds of no speech

# Speaker enrollment filter (default: false)
# When enabled, only creates conversations when enrolled speakers are detected
# Requires speaker recognition service to be running and speakers to be enrolled
# Set to "true" to enable, "false" or omit to disable
RECORD_ONLY_ENROLLED_SPEAKERS=true

# ========================================
# DATABASE CONFIGURATION
# ========================================

# MongoDB for conversations and user data (defaults to mongodb://mongo:27017)
MONGODB_URI=mongodb://mongo:27017

# Qdrant for vector memory storage (defaults to qdrant)
QDRANT_BASE_URL=qdrant


# ========================================
# MEMORY PROVIDER CONFIGURATION
# ========================================

# Memory Provider: "friend_lite" (default) or "openmemory_mcp"
# 
# Friend-Lite (default): In-house memory system with full control
# - Custom LLM-powered extraction with individual fact storage
# - Smart deduplication and memory updates (ADD/UPDATE/DELETE)
# - Direct Qdrant vector storage
# - No external dependencies
#
# OpenMemory MCP: Delegates to external OpenMemory MCP server
# - Professional memory processing with cross-client compatibility
# - Works with Claude Desktop, Cursor, Windsurf, etc.
# - Web UI at http://localhost:8765
# - Requires external server setup
#
# See MEMORY_PROVIDERS.md for detailed comparison
MEMORY_PROVIDER=friend_lite

# ----------------------------------------
# OpenMemory MCP Configuration
# (Only needed if MEMORY_PROVIDER=openmemory_mcp)
# ----------------------------------------
# First start the external server:
#   cd extras/openmemory-mcp && docker compose up -d
# 
# OPENMEMORY_MCP_URL=http://host.docker.internal:8765
# OPENMEMORY_CLIENT_NAME=friend_lite
# OPENMEMORY_USER_ID=openmemory
# OPENMEMORY_TIMEOUT=30

# ========================================
# OPTIONAL FEATURES
# ========================================

NEO4J_HOST=neo4j-mem0
NEO4J_USER=neo4j
NEO4J_PASSWORD=

# Debug directory for troubleshooting
DEBUG_DIR=./data/debug_dir

# Ngrok for external access (if using ngrok from docker-compose)
# NGROK_AUTHTOKEN=

# Speaker recognition service
# HF_TOKEN=
# SPEAKER_SERVICE_URL=http://speaker-recognition:8001

# Audio processing settings
# NEW_CONVERSATION_TIMEOUT_MINUTES=1.5
# AUDIO_CROPPING_ENABLED=true
# MIN_SPEECH_SEGMENT_DURATION=1.0
# CROPPING_CONTEXT_PADDING=0.1

# ========================================
# SPEECH-DRIVEN CONVERSATIONS CONFIGURATION
# ========================================

# Note: File rotation for long sessions is not yet implemented
# Audio sessions currently create single files that grow until the session ends


# ========================================
# PUBLIC ACCESS CONFIGURATION
# ========================================
# These settings control how the browser accesses the backend for audio playback

# The IP address or hostname where your backend is publicly accessible from the browser
# Examples:
#   - For local development: localhost or 127.0.0.1
#   - For LAN access: your machine's IP (e.g., 192.168.1.100)
#   - For VPN/Tailscale access: your VPN IP (e.g., 100.64.x.x for Tailscale)
#   - For internet access: your domain or public IP (e.g., friend.example.com)
# Note: This must be accessible from your browser, not from the Docker container
HOST_IP=localhost

# Backend API port (where audio files are served)
BACKEND_PUBLIC_PORT=8000

# WebUI port (defaults to 5173 for Vite dev server)
WEBUI_PORT=5173

# CORS origins (comma-separated list of allowed origins for browser requests)
# Note: Tailscale IPs (100.x.x.x) are automatically supported via regex
# For HTTPS access, add HTTPS origins after running ./init.sh <tailscale-ip>
# Examples:
#   - Local HTTP: http://localhost:5173,http://127.0.0.1:5173
#   - Local HTTPS: https://localhost,https://127.0.0.1
#   - Tailscale HTTPS: https://100.x.x.x
#   - Custom: http://192.168.1.100:5173,https://192.168.1.100
CORS_ORIGINS=http://localhost:5173,http://localhost:3000,http://127.0.0.1:5173,http://127.0.0.1:3000

# Memory settings
# MEM0_TELEMETRY=False

# Langfuse settings
LANGFUSE_PUBLIC_KEY=""
LANGFUSE_SECRET_KEY=""
LANGFUSE_HOST="http://x.x.x.x:3002"
LANGFUSE_ENABLE_TELEMETRY=False