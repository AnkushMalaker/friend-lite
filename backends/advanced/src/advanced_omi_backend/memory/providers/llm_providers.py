"""LLM provider implementations for memory service.

This module provides concrete implementations of LLM providers for:
- OpenAI (GPT models)
- Ollama (local models)

Each provider handles memory extraction, embedding generation, and
memory action proposals using their respective APIs.
"""

import json
import logging
import os
import httpx
from typing import Any, Dict, List, Optional

# TODO: Re-enable spacy when Docker build is fixed
# import spacy

from ..base import LLMProviderBase
from ..prompts import (
    FACT_RETRIEVAL_PROMPT,
    build_update_memory_messages,
    get_update_memory_messages,
)
from ..update_memory_utils import (
    extract_assistant_xml_from_openai_response,
    items_to_json,
    parse_memory_xml,
)
from ..utils import extract_json_from_text

memory_logger = logging.getLogger("memory_service")

# TODO: Re-enable spacy when Docker build is fixed
# try:
#     nlp = spacy.load("en_core_web_sm")
# except OSError:
#     # Model not installed, fallback to None
#     memory_logger.warning("spacy model 'en_core_web_sm' not found. Using fallback text chunking.")
#     nlp = None
nlp = None  # Temporarily disabled

def chunk_text_with_spacy(text: str, max_tokens: int = 100) -> List[str]:
    """Split text into chunks using spaCy sentence segmentation.
    max_tokens is the maximum number of words in a chunk.
    """
    # Fallback chunking when spacy is not available
    if nlp is None:
        # Simple sentence-based chunking
        sentences = text.replace('\n', ' ').split('. ')
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for sentence in sentences:
            sentence_tokens = len(sentence.split())
            
            if current_tokens + sentence_tokens > max_tokens and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
                current_tokens = sentence_tokens
            else:
                if current_chunk:
                    current_chunk += ". " + sentence
                else:
                    current_chunk = sentence
                current_tokens += sentence_tokens
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks if chunks else [text]
    
    # Original spacy implementation when available
    doc = nlp(text)
    
    chunks = []
    current_chunk = ""
    current_tokens = 0
    
    for sent in doc.sents:
        sent_text = sent.text.strip()
        sent_tokens = len(sent_text.split())  # Simple word count
        
        if current_tokens + sent_tokens > max_tokens and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = sent_text
            current_tokens = sent_tokens
        else:
            current_chunk += " " + sent_text if current_chunk else sent_text
            current_tokens += sent_tokens
    
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks

class OpenAIProvider(LLMProviderBase):
    """OpenAI LLM provider implementation.
    
    Provides memory extraction, embedding generation, and memory action
    proposals using OpenAI's GPT and embedding models.
    
    Attributes:
        api_key: OpenAI API key
        model: GPT model to use for text generation
        embedding_model: Model to use for embeddings
        base_url: API base URL (for custom endpoints)
        temperature: Sampling temperature for generation
        max_tokens: Maximum tokens in responses
    """

    def __init__(self, config: Dict[str, Any]):
        self.api_key = config["api_key"]
        self.model = config.get("model", "gpt-4")
        self.embedding_model = config.get("embedding_model", "text-embedding-3-small")
        self.base_url = config.get("base_url", "https://api.openai.com/v1")
        self.temperature = config.get("temperature", 0.1)
        self.max_tokens = config.get("max_tokens", 2000)

    async def extract_memories(self, text: str, prompt: str) -> List[str]:
        """Extract memories using OpenAI API with the enhanced fact retrieval prompt.
        
        Args:
            text: Input text to extract memories from
            prompt: System prompt to guide extraction (uses default if empty)
            
        Returns:
            List of extracted memory strings
        """
        try:
            import langfuse.openai as openai
            
            client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            
            # Use the provided prompt or fall back to default
            system_prompt = prompt if prompt.strip() else FACT_RETRIEVAL_PROMPT
            
            # local models can only handle small chunks of input text
            text_chunks = chunk_text_with_spacy(text)
            
            # Process all chunks in sequence, not concurrently
            results = [
                await self._process_chunk(client, system_prompt, chunk, i) 
                for i, chunk in enumerate(text_chunks)
            ]
            
            # Spread list of list of facts into a single list of facts
            cleaned_facts = []
            for result in results:
                memory_logger.info(f"Cleaned facts: {result}")
                cleaned_facts.extend(result)
            
            return cleaned_facts
                
        except Exception as e:
            memory_logger.error(f"OpenAI memory extraction failed: {e}")
            return []
        
    async def _process_chunk(self, client, system_prompt: str, chunk: str, index: int) -> List[str]:
        """Process a single text chunk to extract memories using OpenAI API.
        
        This private method handles the LLM interaction for a single chunk of text,
        sending it to OpenAI's chat completion API with the specified system prompt
        to extract structured memory facts.
        
        Args:
            client: OpenAI async client instance for API communication
            system_prompt: System prompt that guides the memory extraction behavior
            chunk: Individual text chunk to process for memory extraction
            index: Index of the chunk for logging and error tracking purposes
            
        Returns:
            List of extracted memory fact strings from the chunk. Returns empty list
            if no facts are found or if an error occurs during processing.
            
        Note:
            Errors are logged but don't propagate to avoid failing the entire
            memory extraction process.
        """
        try:
            response = await client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": chunk}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                response_format={"type": "json_object"}
            )
            
            facts = (response.choices[0].message.content or "").strip()
            if not facts:
                return []

            return _parse_memories_content(facts)
            
        except Exception as e:
            memory_logger.error(f"Error processing chunk {index}: {e}")
            return []

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using OpenAI API.
        
        Args:
            texts: List of texts to generate embeddings for
            
        Returns:
            List of embedding vectors, one per input text
        """
        try:
            import langfuse.openai as openai
            
            client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            
            response = await client.embeddings.create(
                model=self.embedding_model,
                input=texts
            )
            
            return [data.embedding for data in response.data]
            
        except Exception as e:
            memory_logger.error(f"OpenAI embedding generation failed: {e}")
            raise e

    async def test_connection(self) -> bool:
        """Test OpenAI connection.
        
        Returns:
            True if connection successful, False otherwise
        """
        try:
            # For Ollama, just check if the base URL is reachable
            if os.getenv("LLM_PROVIDER", "openai").lower() == "ollama":
                import httpx
                async with httpx.AsyncClient() as client:
                    # For Ollama, test connection by hitting the /v1/models endpoint
                    response = await client.get(f"{self.base_url}/models")
                    response.raise_for_status()
                return True

            import langfuse.openai as openai
            
            client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            
            await client.models.list()
            return True
            
        except Exception as e:
            memory_logger.error(f"OpenAI connection test failed: {e}")
            return False

    async def propose_memory_actions(
        self,
        retrieved_old_memory: List[Dict[str, str]] | List[str],
        new_facts: List[str],
        custom_prompt: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Use OpenAI chat completion with enhanced prompt to propose memory actions.
        
        Args:
            retrieved_old_memory: List of existing memories for context
            new_facts: List of new facts to process
            custom_prompt: Optional custom prompt to override default
            
        Returns:
            Dictionary containing proposed memory actions
        """
        try:
            import langfuse.openai as openai

            # Generate the complete prompt using the helper function
            memory_logger.debug(f"ðŸ§  Facts passed to prompt builder: {new_facts}")
            update_memory_messages = build_update_memory_messages(
                retrieved_old_memory, 
                new_facts, 
                custom_prompt
            )
            memory_logger.debug(f"ðŸ§  Generated prompt user content: {update_memory_messages[1]['content'][:200]}...")

            client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
            )

            response = await client.chat.completions.create(
                model=self.model,
                messages=update_memory_messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
            )

            content = (response.choices[0].message.content or "").strip()
            if not content:
                return {}

            xml = extract_assistant_xml_from_openai_response(response)
            memory_logger.info(f"OpenAI propose_memory_actions xml: {xml}")
            items = parse_memory_xml(xml)
            memory_logger.info(f"OpenAI propose_memory_actions items: {items}")
            result = items_to_json(items)
            # example {'memory': [{'id': '0', 'event': 'UPDATE', 'text': 'My name is John', 'old_memory': None}}
            memory_logger.info(f"OpenAI propose_memory_actions result: {result}")
            return result

        except Exception as e:
            memory_logger.error(f"OpenAI propose_memory_actions failed: {e}")
            return {}


class OllamaProvider(LLMProviderBase):
    """Ollama LLM provider implementation.
    
    Provides memory extraction, embedding generation, and memory action
    proposals using Ollama's GPT and embedding models.
    
    
    Use the openai provider for ollama with different environment variables
    
    os.environ["OPENAI_API_KEY"] = "ollama"  
    os.environ["OPENAI_BASE_URL"] = "http://localhost:11434/v1"
    os.environ["QDRANT_BASE_URL"] = "localhost"
    os.environ["OPENAI_EMBEDDER_MODEL"] = "erwan2/DeepSeek-R1-Distill-Qwen-1.5B:latest"
    
    """
    pass

def _parse_memories_content(content: str) -> List[str]:
    """
    Parse LLM content to extract memory strings.

    Handles cases where the model returns:
    - A JSON object after </think> with keys like "facts" and "preferences"
    - A plain JSON array of strings
    - Non-JSON text (fallback to single memory)
    """
    try:
        # Try robust extraction first (handles </think> and mixed output)
        parsed = extract_json_from_text(content)
        if isinstance(parsed, dict):
            collected: List[str] = []
            for key in ("facts", "preferences"):
                value = parsed.get(key)
                if isinstance(value, list):
                    collected.extend(
                        [str(item).strip() for item in value if str(item).strip()]
                    )
            # If the dict didn't contain expected keys, try to flatten any list values
            if not collected:
                for value in parsed.values():
                    if isinstance(value, list):
                        collected.extend(
                            [str(item).strip() for item in value if str(item).strip()]
                        )
            if collected:
                return collected
    except Exception:
        # Continue to other strategies
        pass

    # If content includes </think>, try parsing the post-think segment directly
    if "</think>" in content:
        post_think = content.split("</think>", 1)[1].strip()
        if post_think:
            parsed_list = _try_parse_list_or_object(post_think)
            if parsed_list is not None:
                return parsed_list

    # Try to parse the whole content as a JSON list or object
    parsed_list = _try_parse_list_or_object(content)
    if parsed_list is not None:
        return parsed_list

    # Fallback: treat as a single memory string
    return [content] if content else []


def _try_parse_list_or_object(text: str) -> List[str] | None:
    """Try to parse text as JSON list or object and extract strings."""
    try:
        data = json.loads(text)
        if isinstance(data, list):
            return [str(item).strip() for item in data if str(item).strip()]
        if isinstance(data, dict):
            collected: List[str] = []
            for key in ("facts", "preferences"):
                value = data.get(key)
                if isinstance(value, list):
                    collected.extend(
                        [str(item).strip() for item in value if str(item).strip()]
                    )
            if collected:
                return collected
            # As a last attempt, flatten any list values
            for value in data.values():
                if isinstance(value, list):
                    collected.extend(
                        [str(item).strip() for item in value if str(item).strip()]
                    )
            return collected if collected else None
    except Exception:
        return None
