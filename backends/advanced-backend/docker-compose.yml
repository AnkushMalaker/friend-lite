services:
  omi-lite:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./audio_chunks:/app/audio_chunks
    env_file:
      - .env
    depends_on:
      qdrant:
        condition: service_started
      mongo:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/readiness"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    networks:
      - omi-network
  
  streamlit:
    build:
      context: ./webui
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    environment:
      - BACKEND_API_URL=http://friend-backend:8000
      - BACKEND_PUBLIC_URL=http://friend-backend:8000
    depends_on:
      omi-lite:
        condition: service_healthy
      mongo:
        condition: service_started
      qdrant:
        condition: service_started
    volumes:
      - ./webui:/app
    networks:
      - omi-network

  # speaker-recognition:
  #   build:
  #     context: ../../extras/speaker-recognition
  #     dockerfile: Dockerfile
  #   # image: speaker-recognition:latest
  #   ports:
  #     - "8001:8001"
  #   volumes:
  #     # Persist Hugging Face cache (models) between restarts
  #     - ./speaker_model_cache:/models
  #     - ./audio_chunks:/app/audio_chunks  # Share audio chunks with backend
  #     - ./speaker_debug:/app/debug
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   environment:
  #     - HF_HOME=/models
  #     - HF_TOKEN=${HF_TOKEN}
  #     - SIMILARITY_THRESHOLD=0.85
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
    
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   networks:
  #     - omi-network

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333" # gRPC
      - "6334:6334" # HTTP
    volumes:
      - ./qdrant_data:/qdrant/storage # Qdrant will store its data in this named volume
  mongo:
    image: mongo:4.4.18
    container_name: mongo
    ports:
      - "27017:27017"
    volumes:
      - ./mongo_data:/data/db
  ngrok:
    image: ngrok/ngrok:latest
    ports:
      - "4040:4040" # Ngrok web interface
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN}
    command: "http friend-backend:8000 --url=intelligent-hypervisor.ngrok.app"
    depends_on:
      - friend-backend

