services:
  omi-lite:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./audio_chunks:/app/audio_chunks
    env_file:
      - .env
    depends_on:
      qdrant:
        condition: service_started
      mongo:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/readiness"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    networks:
      - omi-network
  
  streamlit:
    build:
      context: ./webui
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    environment:
      - BACKEND_API_URL=http://omi-lite:8000
      - BACKEND_PUBLIC_URL=http://localhost:8000
    depends_on:
      omi-lite:
        condition: service_healthy
      mongo:
        condition: service_started
      qdrant:
        condition: service_started
    volumes:
      - ./webui:/app
    networks:
      - omi-network

  # speaker-recognition:
  #   build:
  #     context: ../../extras/speaker-recognition
  #     dockerfile: Dockerfile
  #   # image: speaker-recognition:latest
  #   ports:
  #     - "8001:8001"
  #   volumes:
  #     # Persist Hugging Face cache (models) between restarts
  #     - ./speaker_model_cache:/models
  #     - ./audio_chunks:/app/audio_chunks  # Share audio chunks with backend
  #     - ./speaker_debug:/app/debug
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   environment:
  #     - HF_HOME=/models
  #     - HF_TOKEN=${HF_TOKEN}
  #     - SIMILARITY_THRESHOLD=0.85
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
    
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   networks:
  #     - omi-network

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333" # gRPC
      - "6334:6334" # HTTP
    volumes:
      - ./qdrant_data:/qdrant/storage # Qdrant will store its data in this named volume
  mongo:
    image: mongo:4.4.18
    container_name: mongo
    ports:
      - "27017:27017"
    volumes:
      - ./mongo_data:/data/db
    networks:
      - omi-network
  # ngrok:
  #   image: ngrok/ngrok:latest
  #   ports:
  #     - "4040:4040" # Ngrok web interface
  #   environment:
  #     - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN}
  #   command: "http omi-lite:8000 --url=${NGROK_URL}"
  #   depends_on:
  #     - omi-lite
  #   networks:
  #     - omi-network

  # neo4j:
  #     image: neo4j:latest
  #     container_name: neo4j-lite
  #     volumes:
  #         - neo4j_data:/data
  #         - neo4j_logs:/logs
  #         - neo4j_config:/config
  #         # - ./plugins:/plugins
  #     env_file:
  #       - .env
  #     environment:
  #       # - NEO4J_AUTH=neo4j/taketheredpillNe0
  #       - NEO4J_server_memory_heap_initial__size=1G
  #       - NEO4J_server_memory_heap_max__size=2G
  #       - NEO4J_server_memory_pagecache_size=1G
  #       - NEO4J_apoc_export_file_enabled=true
  #       - NEO4J_apoc_import_file_enabled=true
  #       - NEO4J_apoc_import_file_use__neo4j__config=true
  #       # - NEO4J_dbms_security_procedures_unrestricted=apoc.*
  #       - NEO4J_PLUGINS=["apoc"]
  #     ports:
  #       - "7474:7474"
  #       - "7687:7687"
  #     restart: always
  #     networks:
  #       - omi-network
  #     healthcheck:
  #       test: ["CMD", "curl", "-f", "http://localhost:7687"]
  #       interval: 30s
  #       timeout: 10s
  #       retries: 5

volumes:
  ollama_data:

networks: 
  omi-network:
    driver: bridge
    external: true