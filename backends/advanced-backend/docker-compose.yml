services:
  friend-backend:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./audio_chunks:/app/audio_chunks
    environment:
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
      - OFFLINE_ASR_TCP_URI=${OFFLINE_ASR_TCP_URI}
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - HF_TOKEN=${HF_TOKEN}
      - SPEAKER_SERVICE_URL=http://speaker-recognition:8001
    depends_on:
      qdrant:
        condition: service_started
      mongo:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/readiness"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
  
  streamlit:
    build:
      context: ./webui
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    environment:
      - BACKEND_API_URL=http://friend-backend:8000
      - BACKEND_PUBLIC_URL=http://localhost:8000
    depends_on:
      friend-backend:
        condition: service_healthy
      mongo:
        condition: service_started
      qdrant:
        condition: service_started
    volumes:
      - ./webui:/app

  # speaker-recognition:
  #   build:
  #     context: ../../extras/speaker-recognition
  #     dockerfile: Dockerfile
  #   # image: speaker-recognition:latest
  #   ports:
  #     - "8001:8001"
  #   volumes:
  #     # Persist Hugging Face cache (models) between restarts
  #     - ./speaker_model_cache:/models
  #     - ./audio_chunks:/app/audio_chunks  # Share audio chunks with backend
  #     - ./speaker_debug:/app/debug
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   environment:
  #     - HF_HOME=/models
  #     - HF_TOKEN=${HF_TOKEN}
  #     - SIMILARITY_THRESHOLD=0.85
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
    
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333" # gRPC
      - "6334:6334" # HTTP
    volumes:
      - ./qdrant_data:/qdrant/storage # Qdrant will store its data in this named volume
  mongo:
    image: mongo:latest
    ports:
      - "27017:27017"
    volumes:
      - ./mongo_data:/data/db
  # ngrok:
  #   image: ngrok/ngrok:latest
  #   ports:
  #     - "4040:4040" # Ngrok web interface
  #   environment:
  #     - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN}
  #   command: "http friend-backend:8000 --url=square-feline-partly.ngrok-free.app"
  #   depends_on:
  #     - friend-backend
