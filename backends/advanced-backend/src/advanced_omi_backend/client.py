"""Simplified ClientState that only manages state, no processing.

This module provides a lightweight ClientState class that tracks conversation
state, timestamps, and speech segments. All processing is handled at the
application level by the ProcessorManager.
"""

import asyncio
import logging
import os
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from wyoming.audio import AudioChunk

from advanced_omi_backend.database import AudioChunksCollectionHelper
from advanced_omi_backend.processors import (
    AudioCroppingItem,
    MemoryProcessingItem,
    get_processor_manager,
)
from advanced_omi_backend.task_manager import get_task_manager

# Get loggers
audio_logger = logging.getLogger("audio_processing")

# Configuration constants
NEW_CONVERSATION_TIMEOUT_MINUTES = float(os.getenv("NEW_CONVERSATION_TIMEOUT_MINUTES", "1.5"))
AUDIO_CROPPING_ENABLED = os.getenv("AUDIO_CROPPING_ENABLED", "true").lower() == "true"


class ClientState:
    """Manages conversation state for a single client connection."""

    def __init__(
        self,
        client_id: str,
        ac_db_collection_helper: AudioChunksCollectionHelper,
        chunk_dir: Path,
        user_id: str,
        user_email: Optional[str] = None,
    ):
        self.client_id = client_id
        self.connected = True
        self.db_helper = ac_db_collection_helper
        self.chunk_dir = chunk_dir

        # Store user data for memory processing
        self.user_id = user_id
        self.user_email = user_email

        # Conversation state tracking
        self.last_transcript_time: Optional[float] = None
        self.conversation_start_time: float = time.time()
        self.current_audio_uuid: Optional[str] = None

        # Speech segment tracking for audio cropping
        self.speech_segments: Dict[str, List[Tuple[float, float]]] = {}
        self.current_speech_start: Dict[str, Optional[float]] = {}

        # NOTE: Removed in-memory transcript storage for single source of truth
        # Transcripts are stored only in MongoDB via TranscriptionManager

        # Track if conversation has been closed
        self.conversation_closed: bool = False

        # Debug tracking
        self.transaction_id: Optional[str] = None

        audio_logger.info(f"Created simplified client state for {client_id}")

    def update_audio_received(self, chunk: AudioChunk):
        """Update state when audio is received."""
        # Check if we should start a new conversation
        if self.should_start_new_conversation():
            asyncio.create_task(self.start_new_conversation())

    def set_current_audio_uuid(self, audio_uuid: str):
        """Set the current audio UUID when processor creates a new file."""
        self.current_audio_uuid = audio_uuid
        self.conversation_closed = False  # Reset for new audio file

    def record_speech_start(self, audio_uuid: str, timestamp: float):
        """Record the start of a speech segment."""
        self.current_speech_start[audio_uuid] = timestamp
        audio_logger.info(f"Recorded speech start for {audio_uuid}: {timestamp}")

    def record_speech_end(self, audio_uuid: str, timestamp: float):
        """Record the end of a speech segment."""
        if (
            audio_uuid in self.current_speech_start
            and self.current_speech_start[audio_uuid] is not None
        ):
            start_time = self.current_speech_start[audio_uuid]
            if start_time is not None:
                if audio_uuid not in self.speech_segments:
                    self.speech_segments[audio_uuid] = []
                self.speech_segments[audio_uuid].append((start_time, timestamp))
                self.current_speech_start[audio_uuid] = None
                duration = timestamp - start_time
                audio_logger.info(
                    f"Recorded speech segment for {audio_uuid}: {start_time:.3f} -> {timestamp:.3f} "
                    f"(duration: {duration:.3f}s)"
                )
        else:
            audio_logger.warning(f"Speech end recorded for {audio_uuid} but no start time found")

    def update_transcript_received(self):
        """Update timestamp when transcript is received (for timeout detection)."""
        self.last_transcript_time = time.time()

    def should_start_new_conversation(self) -> bool:
        """Check if we should start a new conversation based on timeout."""
        if self.last_transcript_time is None:
            return False

        current_time = time.time()
        time_since_last_transcript = current_time - self.last_transcript_time
        timeout_seconds = NEW_CONVERSATION_TIMEOUT_MINUTES * 60

        return time_since_last_transcript > timeout_seconds

    async def close_current_conversation(self):
        """Close the current conversation and queue necessary processing."""
        # Prevent double closure
        if self.conversation_closed:
            audio_logger.debug(
                f"üîí Conversation already closed for client {self.client_id}, skipping"
            )
            return

        self.conversation_closed = True

        if not self.current_audio_uuid:
            audio_logger.info(f"üîí No active conversation to close for client {self.client_id}")
            return

        audio_logger.info(f"üîí Closing conversation {self.current_audio_uuid}")

        # Get processor manager
        processor_manager = get_processor_manager()

        # Close audio file in processor
        await processor_manager.close_client_audio(self.client_id)

        # Queue memory processing if we have required data
        # Memory processor will read transcripts from database directly
        audio_logger.info(f"üí≠ Memory processing check for client {self.client_id}:")
        audio_logger.info(f"    - current_audio_uuid: {self.current_audio_uuid}")
        audio_logger.info(f"    - user_id: {self.user_id}")
        audio_logger.info(f"    - user_email: {self.user_email}")

        if self.current_audio_uuid and self.user_id and self.user_email:
            audio_logger.info(
                f"üí≠ Queuing memory processing for conversation {self.current_audio_uuid}"
            )

            await processor_manager.queue_memory(
                MemoryProcessingItem(
                    client_id=self.client_id,
                    user_id=self.user_id,
                    user_email=self.user_email,
                    audio_uuid=self.current_audio_uuid,
                    db_helper=self.db_helper,
                )
            )
        else:
            audio_logger.warning(f"üí≠ Memory processing skipped - missing required data:")
            audio_logger.warning(f"    - current_audio_uuid: {bool(self.current_audio_uuid)}")
            audio_logger.warning(f"    - user_id: {bool(self.user_id)}")
            audio_logger.warning(f"    - user_email: {bool(self.user_email)}")

        # Queue audio cropping if enabled and we have speech segments
        if AUDIO_CROPPING_ENABLED and self.current_audio_uuid in self.speech_segments:
            speech_segments = self.speech_segments[self.current_audio_uuid]
            if speech_segments:
                # Get the audio file path from processor
                # This assumes the processor follows the same naming convention
                timestamp = int(self.conversation_start_time)
                wav_filename = f"{timestamp}_{self.client_id}_{self.current_audio_uuid}.wav"
                original_path = f"{self.chunk_dir}/{wav_filename}"
                cropped_path = str(original_path).replace(".wav", "_cropped.wav")

                audio_logger.info(
                    f"‚úÇÔ∏è Queuing audio cropping for {self.current_audio_uuid} "
                    f"with {len(speech_segments)} speech segments"
                )

                await processor_manager.queue_cropping(
                    AudioCroppingItem(
                        client_id=self.client_id,
                        user_id=self.user_id,
                        audio_uuid=self.current_audio_uuid,
                        original_path=original_path,
                        speech_segments=speech_segments,
                        output_path=cropped_path,
                    )
                )

            # Clean up segments for this conversation
            del self.speech_segments[self.current_audio_uuid]
            if self.current_audio_uuid in self.current_speech_start:
                del self.current_speech_start[self.current_audio_uuid]

    async def start_new_conversation(self):
        """Start a new conversation by closing current and resetting state."""
        await self.close_current_conversation()

        # Reset conversation state
        self.current_audio_uuid = None
        self.conversation_start_time = time.time()
        self.last_transcript_time = None
        self.conversation_closed = False

        audio_logger.info(
            f"Client {self.client_id}: Started new conversation due to "
            f"{NEW_CONVERSATION_TIMEOUT_MINUTES}min timeout"
        )

    async def disconnect(self):
        """Clean disconnect of client state."""
        if not self.connected:
            return

        self.connected = False
        audio_logger.info(f"Disconnecting client {self.client_id}")

        # Close current conversation
        await self.close_current_conversation()

        # Cancel any tasks for this client
        task_manager = get_task_manager()
        await task_manager.cancel_tasks_for_client(self.client_id)

        # Clean up state
        self.speech_segments.clear()
        self.current_speech_start.clear()

        audio_logger.info(f"Client {self.client_id} disconnected and cleaned up")
