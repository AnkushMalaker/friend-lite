# Speaker Recognition System

A comprehensive speaker recognition system with web-based UI for audio annotation, speaker enrollment, and data management.

## üöÄ Quick Start

### Prerequisites
- Docker and Docker Compose
- Hugging Face account (for model access)
- 8GB+ RAM, 10GB+ disk space

### 1. Set up your Hugging Face token
```bash
export HF_TOKEN="your_huggingface_token"
```
Get your token from https://huggingface.co/settings/tokens

### 2. Start the system
```bash
cd extras/speaker-recognition
docker-compose up --build -d
```

This starts two services:
- **FastAPI backend** on http://localhost:8001 (speaker recognition API)
- **Streamlit web UI** on http://localhost:8501 (main interface)

### 3. Access the Web UI
Open http://localhost:8501 in your browser

### 4. Get Started
1. **Create a user** using the sidebar
2. **Upload audio** in the "Audio Viewer" page
3. **Annotate segments** in the "Annotation" page
4. **Enroll speakers** in the "Enrollment" page
5. **Manage & export** data in the "Speakers" page

## üéØ What You Can Do

- **üìÅ Upload & Visualize**: Interactive waveforms, spectrograms, segment selection
- **üìù Annotate Audio**: Label speaker segments, handle unknown speakers
- **üë§ Enroll Speakers**: Register speakers with quality assessment
- **üë• Manage Speakers**: View statistics, compare quality, bulk operations
- **üì§ Export Data**: Download audio in organized folders or concatenated files
- **üìä Analytics**: Track quality trends and system usage

## üõ†Ô∏è System Architecture

- **Multi-user Support**: Each user manages their own speaker data
- **SQLite Database**: Local storage for annotations, speakers, and sessions  
- **Quality Assessment**: Automatic audio quality scoring with recommendations
- **Export Formats**: 
  - Concatenated audio (max 10min per file)
  - Segmented files: `./exported_data/speaker-1/audio001.wav`
  - Metadata and annotations as JSON

## üìñ Deepgram Terminology in Speaker Recognition Context

When importing Deepgram transcripts, you'll encounter different data structures:

### Words
- **What**: Individual word-level transcription data with precise timestamps
- **Use**: Most accurate for speaker boundaries and timing
- **Example**: `{"word": "hello", "start": 1.23, "end": 1.45, "speaker": 0}`

### Segments  
- **What**: Groups of consecutive words from the same speaker
- **Use**: Natural speech chunks for annotation and training
- **Generated by**: DeepgramParser groups words by speaker changes
- **Example**: Speaker 1 talks from 0-5s, then Speaker 2 from 5-10s

### Paragraphs/Sentences
- **What**: Deepgram's paragraph-level grouping based on pauses/context
- **Use**: Better for readability but less precise for speaker boundaries
- **Note**: May combine multiple speakers if they talk quickly
- **Example**: A paragraph might span 30-60 seconds with multiple speakers

### Best Practice
The speaker recognition system uses **word-level data** to create segments because:
- Exact speaker change points (critical for training)
- No overlapping speakers in a segment
- Accurate timing for audio extraction

When you see mismatched text in the UI, it's often because:
- Old imports used paragraph-level data
- Display shows cached/different parsing method
- Solution: Re-import using current word-level parser

## üìö Documentation

For detailed documentation, API reference, and advanced usage:
- **[README.detailed.md](README.detailed.md)** - Comprehensive guide
- **[plan.md](plan.md)** - Implementation details

## üîß Configuration

Environment variables:
```bash
# Required
HF_TOKEN="your_token"                    # Required: Hugging Face token for PyAnnote models

# Speaker Service Configuration  
SPEAKER_SERVICE_HOST="0.0.0.0"          # Speaker service bind host
SPEAKER_SERVICE_PORT="8085"             # Speaker service port (default: 8085)
SPEAKER_SERVICE_URL="http://speaker-service:8085"  # URL for internal Docker communication
SIMILARITY_THRESHOLD="0.15"             # Speaker similarity threshold (0.1-0.3 typical for ECAPA-TDNN)

# Streamlit Web UI Configuration
STREAMLIT_HOST="0.0.0.0"               # Web UI bind host  
STREAMLIT_PORT="8502"                   # Web UI port (default: 8502)

# Optional
DEEPGRAM_API_KEY="your_key"             # For transcript import features
DEV="false"                             # Enable development mode with reload
```

Copy `.env.template` to `.env` and configure your settings:
```bash
cp .env.template .env
# Edit .env with your configuration
```

## üö® Troubleshooting

**Can't access the web UI?**
- Check if services are running: `docker-compose ps`
- View logs: `docker-compose logs web-ui`

**Speaker service not responding?**
- Check backend logs: `docker-compose logs speaker-service`
- Verify HF_TOKEN is set correctly

**Models not downloading?**
- Ensure HF_TOKEN has access to PyAnnote models
- Check network connection and disk space

## üîÑ Development

For local development without Docker:
```bash
# Terminal 1 - Backend
uv sync
uv run python speaker_service.py

# Terminal 2 - Web UI  
uv run streamlit run web_ui.py
```

## API Endpoints

### Health Check
```bash
GET /health
```
**Response:**
```json
{
  "status": "ok",
  "device": "cuda",
  "speakers": 5
}
```

### Speaker Enrollment - Single File
```bash
POST /enroll/upload
Content-Type: multipart/form-data
```
**Form Fields:**
- `file`: Audio file (WAV/FLAC, max 3 minutes)
- `speaker_id`: Unique speaker identifier
- `speaker_name`: Speaker display name  
- `start`: Start time in seconds (optional)
- `end`: End time in seconds (optional)

**Response:**
```json
{
  "updated": false,
  "speaker_id": "john_doe"
}
```

### Speaker Enrollment - Batch
```bash
POST /enroll/batch
Content-Type: multipart/form-data
```
**Form Fields:**
- `files`: Multiple audio files for same speaker
- `speaker_id`: Unique speaker identifier
- `speaker_name`: Speaker display name

**Response:**
```json
{
  "updated": false,
  "speaker_id": "john_doe",
  "num_segments": 3,
  "num_files": 3
}
```

### Speaker Diarization and Identification
```bash
POST /diarize-and-identify
Content-Type: multipart/form-data
```
**Form Fields:**
- `file`: Audio file for processing
- `min_duration`: Minimum segment duration in seconds (optional, default: 0.5)
- `similarity_threshold`: Override default threshold (optional)
- `identify_only_enrolled`: Return only identified speakers (optional, default: false)

**Response:**
```json
{
  "segments": [
    {
      "speaker": "SPEAKER_00",
      "start": 1.234,
      "end": 5.678,
      "duration": 4.444,
      "identified_as": "John Doe",
      "identified_id": "john_doe",
      "confidence": 0.892,
      "status": "identified"
    }
  ],
  "summary": {
    "total_duration": 120.5,
    "num_segments": 15,
    "num_diarized_speakers": 3,
    "identified_speakers": ["John Doe", "Jane Smith"],
    "unknown_speakers": ["SPEAKER_02"],
    "similarity_threshold": 0.15,
    "filtered": false
  }
}
```

### List Speakers
```bash
GET /speakers
```
**Response:**
```json
{
  "speakers": [
    {
      "id": "john_doe",
      "name": "John Doe"
    }
  ]
}
```

### Reset All Speakers
```bash
POST /speakers/reset
```
**Response:**
```json
{
  "reset": true
}
```

### Delete Speaker
```bash
DELETE /speakers/{speaker_id}
```
**Response:**
```json
{
  "deleted": true
}
```

## Integration with Advanced Backend

The advanced backend communicates with this service through the `client.py` module, which provides both async and sync interfaces for backward compatibility.

## Laptop Client

The `laptop_client.py` provides a command-line interface for recording from your microphone and interacting with the speaker recognition service.

### Setup

1. **Install dependencies**:
   ```bash
   pip install -r requirements-laptop.txt
   ```

2. **Start the speaker service** (using Docker or locally):
   ```bash
   docker-compose up
   # or
   python speaker_service.py
   ```

### Usage

#### Enroll a Speaker
Record 10 seconds of audio to enroll a new speaker:
```bash
python laptop_client.py enroll --speaker-id "john" --speaker-name "John Doe" --duration 10
```

#### Identify a Speaker
Record 5 seconds of audio to identify who is speaking:
```bash
python laptop_client.py identify --duration 5
```

#### Verify a Speaker
Check if the voice matches a specific enrolled speaker:
```bash
python laptop_client.py verify --speaker-id "john" --duration 3
```

#### List Enrolled Speakers
```bash
python laptop_client.py list
```

#### Remove a Speaker
```bash
python laptop_client.py remove --speaker-id "john"
```

### Options

- `--service-url`: Change the service URL (default: http://localhost:8001)
- `--duration`: Recording duration in seconds
- `--speaker-id`: Unique identifier for speaker
- `--speaker-name`: Display name for speaker

### Example Workflow

1. Start the service:
   ```bash
   docker-compose up
   ```

2. Enroll yourself:
   ```bash
   python laptop_client.py enroll --speaker-id "myself" --speaker-name "My Name" --duration 15
   ```

3. Test identification:
   ```bash
   python laptop_client.py identify --duration 5
   ```

## Laptop Client

A command-line client (`laptop_client.py`) that can record from your microphone and interact with the speaker recognition service.

### Setup for Laptop Client

The laptop client requires PyAudio for microphone access:

```bash
# On Ubuntu/Debian
sudo apt-get install portaudio19-dev python3-pyaudio

# On macOS
brew install portaudio
pip install pyaudio

# On Windows
pip install pyaudio
```

### Usage Examples

```bash
# Start the speaker service first
docker-compose up -d

# Enroll a new speaker (records 10 seconds)
python laptop_client.py enroll --speaker-id "john" --speaker-name "John Doe" --duration 10

# Identify a speaker (records 5 seconds)
python laptop_client.py identify --duration 5

# Verify against a specific speaker (records 3 seconds)
python laptop_client.py verify --speaker-id "john" --duration 3

# List all enrolled speakers
python laptop_client.py list

# Remove a speaker
python laptop_client.py remove --speaker-id "john"

# Use different service URL
python laptop_client.py --service-url "http://192.168.1.100:8001" identify
```

### Laptop Client Features

- **Live Microphone Recording**: Records directly from your system microphone
- **Automatic Cleanup**: Temporary audio files are automatically cleaned up
- **Service Health Checks**: Verifies the speaker service is online before operations
- **Real-time Feedback**: Shows recording progress and results with emojis
- **Error Handling**: Graceful handling of network and audio errors

## Integration with Advanced Backend

The advanced backend communicates with this service through the `client.py` module, which provides both async and sync interfaces for backward compatibility.

## Performance Notes

- First inference may be slow due to model loading
- GPU memory usage scales with model size (~2-4GB)
- Audio files should be accessible from both services (use shared volumes)
- Microphone recording uses 16kHz sample rate for optimal compatibility
- Microphone recording requires `pyaudio` and proper audio device setup 