[project]
name = "speaker-omni-experimental"
version = "0.1.0"
description = "Experimental Qwen2.5-Omni speaker recognition with data preparation tools"
requires-python = ">=3.10"

dependencies = [
    # Core ML Framework for Qwen2.5-Omni
    "torch>=2.0.0",
    "transformers>=4.35.0",
    
    # Audio Processing
    "soundfile>=0.12.0",
    "librosa>=0.10.0",
    "pydub>=0.25.1",
    
    # YouTube downloading and transcription
    "yt-dlp>=2025.7.21",
    "deepgram-sdk>=4.7.0",
    
    # Configuration and Utilities
    "PyYAML>=6.0",
    "numpy>=1.21.0",
    "scipy>=1.10.0",
    
    # Data processing for speaker analysis
    "pandas>=2.2.0",
    "scikit-learn>=1.4.0",
    
    # CLI and async support
    "click>=8.1.0",
    "aiohttp>=3.8.0",
    
    # Audio quality assessment
    "matplotlib>=3.8.0",  # For visualization during development
]

[dependency-groups]
# GPU acceleration for faster inference
gpu = [
    "torch[cuda]>=2.0.0",
    "flash-attn>=2.0.0",  # Flash attention for memory efficiency
    "accelerate>=0.21.0",  # Multi-GPU support
]

# Development tools
dev = [
    "black>=25.1.0",
    "isort>=6.0.1",
    "pytest>=7.0.0",
    "ipython>=8.0.0",
]

[tool.uv]
# Install dev dependencies by default
default-groups = ["dev"]

[project.scripts]
# CLI entry points for data preparation
data-prep = "data_preparation:main"
qwen-diarize = "qwen_speaker_diarizer:main"

[tool.setuptools.packages.find]
where = ["."]
include = ["*.py"]

[tool.isort]
profile = "black"

[tool.black]
line-length = 100
target-version = ['py310']